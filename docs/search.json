[
  {
    "objectID": "EDA.html#final-project-for-st558",
    "href": "EDA.html#final-project-for-st558",
    "title": "EDA",
    "section": "Final project for ST558",
    "text": "Final project for ST558"
  },
  {
    "objectID": "EDA.html#introduction",
    "href": "EDA.html#introduction",
    "title": "EDA",
    "section": "1. Introduction",
    "text": "1. Introduction\n\n1.1 Dataset\nDiabetes _ binary _ health _ indicators Dataset is a clean dataset of 253,680 survey responses to the CDC’s BRFSS2015. The target variable Diabetes_binary has 2 classes. 0 is for no diabetes, and 1 is for prediabetes or diabetes. This dataset has 21 feature variables which have been reported can influencing diabetes disease and other chronic health conditions based on diabetes disease research.\nAmong all variables, I tried to select features, which I thought could be important risk factors for diabetes and other chronic illnesses. The selected features from the BRFSS 2015 dataset are:\nResponse Variable / Dependent Variable:\nDiabetes_binary O for no diabetes, 1 for prediabetes.\nIndependent Variables:\nHighBP, High Blood Pressure\n0 = no high BP 1 = high BP\nHighChol, High Cholesterol\n0 = no high cholesterol 1 = high cholesterol\nBMI\nBody Mass Index (BMI)\nSmoking\nHave you smoked at least 100 cigarettes in your entire life? [Note: 5 packs = 100 cigarettes] 0 = no 1 = yes\nPhysActivity, Physical Activity\nAdults who reported doing physical activity in past 30 days - not including job 0 = no 1 = yes\nVeggies Consume Vegetables 1 or more times per day 0 = no 1 = yes\nHvyAlcoholConsump, Alcohol Consumption\nHeavy drinkers (adult men having more than 14 drinks per week and adult women having more than 7 drinks per week) 0 = no 1 = yes\nDiffWalk Do you have serious difficulty walking or climbing stairs? 0 = no 1 = yes\nSex Indicate sex of respondent.\nAge 13-level age category.\n\n\n1.2 EDA and modeling\nExploratory Data Analysis (EDA) is a crucial step in the data science workflow to process this dataset. The primary purpose of EDA in this project including:\n\nIdentify Missing Values: Determine the extent and pattern of missing data, which helps in deciding how to handle them (e.g., imputation, deletion).\nExamine Relationships: Explore correlations and interactions between variables to uncover potential predictors and multicollinearity issues.\n\nAfter EDA, we will employ several models for this dataset. The ultimate goal of modeling is to develop a predictive models that accurately captures the relationship between input features (response variables) and the target variable (diabetes here) to make reliable predictions on new, unseen data."
  },
  {
    "objectID": "EDA.html#get-the-data",
    "href": "EDA.html#get-the-data",
    "title": "EDA",
    "section": "2. Get the Data",
    "text": "2. Get the Data\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidyr)\nlibrary(ggplot2)\n\nFirst, we read in the data and check the variables via str().\n\ndf &lt;- read.csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\nstr(df)\n\n'data.frame':   253680 obs. of  22 variables:\n $ Diabetes_binary     : num  0 0 0 0 0 0 0 0 1 0 ...\n $ HighBP              : num  1 0 1 1 1 1 1 1 1 0 ...\n $ HighChol            : num  1 0 1 0 1 1 0 1 1 0 ...\n $ CholCheck           : num  1 0 1 1 1 1 1 1 1 1 ...\n $ BMI                 : num  40 25 28 27 24 25 30 25 30 24 ...\n $ Smoker              : num  1 1 0 0 0 1 1 1 1 0 ...\n $ Stroke              : num  0 0 0 0 0 0 0 0 0 0 ...\n $ HeartDiseaseorAttack: num  0 0 0 0 0 0 0 0 1 0 ...\n $ PhysActivity        : num  0 1 0 1 1 1 0 1 0 0 ...\n $ Fruits              : num  0 0 1 1 1 1 0 0 1 0 ...\n $ Veggies             : num  1 0 0 1 1 1 0 1 1 1 ...\n $ HvyAlcoholConsump   : num  0 0 0 0 0 0 0 0 0 0 ...\n $ AnyHealthcare       : num  1 0 1 1 1 1 1 1 1 1 ...\n $ NoDocbcCost         : num  0 1 1 0 0 0 0 0 0 0 ...\n $ GenHlth             : num  5 3 5 2 2 2 3 3 5 2 ...\n $ MentHlth            : num  18 0 30 0 3 0 0 0 30 0 ...\n $ PhysHlth            : num  15 0 30 0 0 2 14 0 30 0 ...\n $ DiffWalk            : num  1 0 1 0 0 0 0 1 1 0 ...\n $ Sex                 : num  0 0 0 0 0 1 0 0 0 1 ...\n $ Age                 : num  9 7 9 11 11 10 9 11 9 8 ...\n $ Education           : num  4 6 4 3 5 6 6 4 5 4 ...\n $ Income              : num  3 1 8 6 4 8 7 4 1 3 ...\n\n\n\nThe data includes 253680 observations and 22 varibales.\nWe can note that all the variables are category variables except BMI, which is the only one numeric variable.\nWe need to covert the category variables to factors with meaningful names."
  },
  {
    "objectID": "EDA.html#clean-the-data",
    "href": "EDA.html#clean-the-data",
    "title": "EDA",
    "section": "3. Clean the data",
    "text": "3. Clean the data\n\n3.1 Select variables\nFirstly, I will select my interest variables which will be used in hte following investigation, and convert the category variables into factors.\nIn order to do this part, I referenced the codebook which says what each column/feature/question is: https://www.cdc.gov/brfss/annual_data/2015/pdf/codebook15_llcp.pdf\n\ndiab_df &lt;- df |&gt;\n  select(Diabetes_binary, HighBP, HighChol, Smoker, BMI, Veggies, HvyAlcoholConsump, DiffWalk, Sex, Age) |&gt;\n  mutate(Diabetes_binary = factor(Diabetes_binary,\n                                  levels = c(0, 1),\n                                  labels = c(\"no_diabetes\", \"prediabetes\"))) |&gt;\n  mutate(HighBP = factor(HighBP,\n                                  levels = c(0, 1),\n                                  labels = c(\"no_high\", \"high_BP\"))) |&gt;\n  mutate(HighChol = factor(HighChol,\n                                  levels = c(0, 1),\n                                  labels = c(\"no_high_cholesterol\", \"high_cholesterol\"))) |&gt;\n\n  mutate(Smoker = factor(Smoker,\n                                  levels = c(0, 1),\n                                  labels = c(\"no\", \"yes\"))) |&gt;\n  mutate(Veggies = factor(Veggies,\n                                  levels = c(0, 1),\n                                  labels = c(\"no\", \"yes\"))) |&gt;\n  mutate(HvyAlcoholConsump = factor(HvyAlcoholConsump,\n                                  levels = c(0, 1),\n                                  labels = c(\"no\", \"yes\"))) |&gt;\n  mutate(DiffWalk = factor(DiffWalk,\n                                  levels = c(0, 1),\n                                  labels = c(\"no\", \"yes\"))) |&gt;\n  mutate(Sex = factor(Sex,\n                                  levels = c(0, 1),\n                                  labels = c(\"female\", \"male\"))) |&gt;\n  mutate(Age = factor(Age,\n                                  levels = c(1:13),\n                                  labels = c(\"Age_18_to_24\", \"Age_25_to_29\", \"Age_30_to_34\", \"Age_35_to_39\", \"Age_40_to_44\", \"Age_45_to_49\", \"Age_50_to_54\", \"Age_55_to_59\", \"Age_60_to_64\", \"Age_65_to_69\", \"Age_70_to_74\", \"Age_75_to_79\", \"Age_80_or_older\")))\n\ndim(diab_df)\n\n[1] 253680     10\n\nstr(diab_df)\n\n'data.frame':   253680 obs. of  10 variables:\n $ Diabetes_binary  : Factor w/ 2 levels \"no_diabetes\",..: 1 1 1 1 1 1 1 1 2 1 ...\n $ HighBP           : Factor w/ 2 levels \"no_high\",\"high_BP\": 2 1 2 2 2 2 2 2 2 1 ...\n $ HighChol         : Factor w/ 2 levels \"no_high_cholesterol\",..: 2 1 2 1 2 2 1 2 2 1 ...\n $ Smoker           : Factor w/ 2 levels \"no\",\"yes\": 2 2 1 1 1 2 2 2 2 1 ...\n $ BMI              : num  40 25 28 27 24 25 30 25 30 24 ...\n $ Veggies          : Factor w/ 2 levels \"no\",\"yes\": 2 1 1 2 2 2 1 2 2 2 ...\n $ HvyAlcoholConsump: Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ DiffWalk         : Factor w/ 2 levels \"no\",\"yes\": 2 1 2 1 1 1 1 2 2 1 ...\n $ Sex              : Factor w/ 2 levels \"female\",\"male\": 1 1 1 1 1 2 1 1 1 2 ...\n $ Age              : Factor w/ 13 levels \"Age_18_to_24\",..: 9 7 9 11 11 10 9 11 9 8 ...\n\n\n\nAt this point, I got data includes 253680 observations and 10 variables.\n\n\n\n3.2 Check missing values\n\n# check missing value\nsum_na &lt;- function(column) {\n  sum(is.na(column))\n}\n\nna_counts &lt;- diab_df |&gt;\n  summarise(across(everything(), sum_na))\nna_counts\n\n  Diabetes_binary HighBP HighChol Smoker BMI Veggies HvyAlcoholConsump DiffWalk\n1               0      0        0      0   0       0                 0        0\n  Sex Age\n1   0   0\n\n\n\nThere’s no missing value data found."
  },
  {
    "objectID": "EDA.html#summary-the-data",
    "href": "EDA.html#summary-the-data",
    "title": "EDA",
    "section": "4. Summary the data",
    "text": "4. Summary the data\n\n4.1 Covariation between HighBP with Diabetes_binary\n\nggplot(data = diab_df, aes(x = Diabetes_binary, y = HighBP)) +\n  geom_count(aes(size=..prop.., group=Diabetes_binary), colour=\"blue\") \n\nWarning: The dot-dot notation (`..prop..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(prop)` instead.\n\n\n\n\n\n\n\n\n\n\nFrom the bubbleplot, we can note that in the group of no diabetes, lager portion of case with no high BP, while high BP dominates the group of prediabetes, suggesting the potential positive relationship between the two variables.\n\n\n\n4.2 Covariation between HighChol with Diabetes_binary\n\nggplot(data = diab_df, aes(x = Diabetes_binary, y = HighChol)) +\n  geom_count(aes(size=..prop.., group=Diabetes_binary), colour=\"blue\") \n\n\n\n\n\n\n\n\n\nSimilar with HighBP, we can note that in the group of no diabetes, lager portion of case with no high cholesterol, while high cholesterol dominates the group of prediabetes, suggesting the potential positive relationship between the two variables.\n\n\n\n4.3 Covariation between Smoker with Diabetes_binary\n\nggplot(data = diab_df, aes(x = Diabetes_binary, y = Smoker)) +\n  geom_count(aes(size=..prop.., group=Diabetes_binary), colour=\"blue\") \n\n\n\n\n\n\n\n\n\nFrom the bubbleplot, we can note that in the group of no diabetes, lager portion of case with non smoker, while in the group of prediabetes, there are more smoker than non smoker, suggesting the potential positive relationship between the two variables.\n\n\n\n4.4 Covariation between BMI with Diabetes_binary\n\nggplot(diab_df, aes(x = Diabetes_binary, y = BMI)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nThe boxplot showed that the media value of BMI in prediabetes group is bigger than no diabetes group.\n\n\n\n4.5 Covariation between Veggies with Diabetes_binary\n\nggplot(data = diab_df, aes(x = Diabetes_binary, y = Veggies)) +\n  geom_count(aes(size=..prop.., group=Diabetes_binary), colour=\"red\") \n\n\n\n\n\n\n\n\n\nVeggies case dominate both no diabetes and prediabetes groups, so the two variables do not show significant correlation.\n\n\n\n4.6 Covariation between HvyAlcoholConsump with Diabetes_binary\n\nggplot(data = diab_df, aes(x = Diabetes_binary, y = HvyAlcoholConsump)) +\n  geom_count(aes(size=..prop.., group=Diabetes_binary), colour=\"red\") \n\n\n\n\n\n\n\n\n\nSimilar with previous Veggies, HvyAlcoholConsump did not increase the occurrence of prediabetes.\n\n\n\n4.7 Covariation between DiffWalk with Diabetes_binary\n\nggplot(data = diab_df, aes(x = Diabetes_binary, y = DiffWalk)) +\n  geom_count(aes(size=..prop.., group=Diabetes_binary), colour=\"blue\") \n\n\n\n\n\n\n\n\n\nWe can note that in no diabetes group, most case do not have diffwalk, while in prediabetes, the portion of diffwalk increased, but still less than no-diffwalk samples.\n\n\n\n4.8 Covariation between Sex with Diabetes_binary\n\nggplot(data = diab_df, aes(x = Diabetes_binary, y = Sex)) +\n  geom_count(aes(size=..prop.., group=Diabetes_binary), colour=\"blue\") \n\n\n\n\n\n\n\n\n\nThe plot indicates that the female dominates no diabetes group, while the portion of male increased in prediabetes group, which is almost half to half with female cases.\n\n\n\n4.9 Covariation between Age with Diabetes_binary\n\nggplot(data = diab_df, aes(x = Diabetes_binary, y = Age)) +\n  geom_count(aes(size=..prop.., group=Diabetes_binary), colour=\"blue\") \n\n\n\n\n\n\n\n\n\nIn general, as age getting bigger, the portion getting bigger first and then decreased in both diabetes groups. However, compared to no diabetes group, the younger people have less portion in prediabetes group, and the largest portion showed in age 65-69.\n\nBased on the covariation analysis, some vairables do not show significant relationship with target variable. We will drop them in following modeling analysis, including Veggies, HvyAlcoholConsump and DiffWalk.\n\ndiab_df &lt;- df |&gt;\n  select(Diabetes_binary, HighBP, HighChol, Smoker, BMI, Veggies, HvyAlcoholConsump, DiffWalk, Sex, Age) |&gt;\n  mutate(Diabetes_binary = factor(Diabetes_binary,\n                                  levels = c(0, 1),\n                                  labels = c(\"no_diabetes\", \"prediabetes\"))) |&gt;\n  mutate(HighBP = factor(HighBP,\n                                  levels = c(0, 1),\n                                  labels = c(\"no_high\", \"high_BP\"))) |&gt;\n  mutate(HighChol = factor(HighChol,\n                                  levels = c(0, 1),\n                                  labels = c(\"no_high_cholesterol\", \"high_cholesterol\"))) |&gt;\n\n  mutate(Smoker = factor(Smoker,\n                                  levels = c(0, 1),\n                                  labels = c(\"no\", \"yes\"))) |&gt;\n\n  mutate(Sex = factor(Sex,\n                                  levels = c(0, 1),\n                                  labels = c(\"female\", \"male\"))) |&gt;\n  mutate(Age = factor(Age,\n                                  levels = c(1:13),\n                                  labels = c(\"Age_18_to_24\", \"Age_25_to_29\", \"Age_30_to_34\", \"Age_35_to_39\", \"Age_40_to_44\", \"Age_45_to_49\", \"Age_50_to_54\", \"Age_55_to_59\", \"Age_60_to_64\", \"Age_65_to_69\", \"Age_70_to_74\", \"Age_75_to_79\", \"Age_80_or_older\")))"
  },
  {
    "objectID": "EDA.html#click-here-for-the-modeling-page",
    "href": "EDA.html#click-here-for-the-modeling-page",
    "title": "EDA",
    "section": "Click here for the Modeling Page",
    "text": "Click here for the Modeling Page"
  },
  {
    "objectID": "Modeling.html#final-project-for-st558",
    "href": "Modeling.html#final-project-for-st558",
    "title": "Modeling",
    "section": "Final project for ST558",
    "text": "Final project for ST558"
  },
  {
    "objectID": "Modeling.html#introduction",
    "href": "Modeling.html#introduction",
    "title": "Modeling",
    "section": "1. Introduction",
    "text": "1. Introduction\n\n1.1 Dataset\nDiabetes _ binary _ health _ indicators Dataset is a clean dataset of 253,680 survey responses to the CDC’s BRFSS2015. The target variable Diabetes_binary has 2 classes. 0 is for no diabetes, and 1 is for prediabetes or diabetes. This dataset has 21 feature variables which have been reported can influencing diabetes disease and other chronic health conditions based on diabetes disease research.\nAmong all variables, I tried to select features, which I thought could be important risk factors for diabetes and other chronic illnesses.\n\n\n1.2 Modeling\nThe ultimate goal of modeling is to develop a predictive models that accurately captures the relationship between input features (response variables) and the target variable (diabetes here) to make reliable predictions on new, unseen data.\nAs the target variable in this project is a binary classification variable, we will use three type of models:\n\nLogistic Regression Logistic regression is a linear model used for binary classification. It predicts the probability that an observation belongs to a particular class.\nClassification Trees Classification trees split the data into subsets based on the value of input features, creating a tree-like model of decisions.\nRandom Forest Random Forest is an ensemble method that combines multiple decision trees, each trained on a random subset of the data and features.\n\n\n\n1.3 LossLog metric\nIn this study, the logLoss will be used as metric to select the best models. LogLoss, also known as logistic loss or cross-entropy loss, is a performance metric for evaluating the predictions of classification models, particularly in binary classification tasks. It measures the accuracy of the probabilistic predictions by comparing the predicted probabilities to the actual class labels. Mathematically, log loss is defined as the negative average of the log of the predicted probabilities assigned to the true classes. Unlike accuracy, which only considers whether the predicted class matches the actual class, log loss takes into account the confidence of the predictions. This is particularly important when dealing with imbalanced datasets or when the cost of misclassification varies. By penalizing incorrect predictions more heavily when the model is confident about them, log loss provides a more nuanced and informative evaluation of model performance. This helps in understanding not just whether the model is correct, but how confident it is in its predictions, leading to better model calibration and more reliable decision-making."
  },
  {
    "objectID": "Modeling.html#spliting-data",
    "href": "Modeling.html#spliting-data",
    "title": "Modeling",
    "section": "2. Spliting data",
    "text": "2. Spliting data\nIn this part, I will split the data into training and test sets at 70/30.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidyr)\nlibrary(caret)\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\nlibrary(rpart)\nlibrary(gbm)\n\nLoaded gbm 2.2.2\nThis version of gbm is no longer under development. Consider transitioning to gbm3, https://github.com/gbm-developers/gbm3\n\nlibrary(Metrics)\n\n\nAttaching package: 'Metrics'\n\nThe following objects are masked from 'package:caret':\n\n    precision, recall\n\nlibrary(randomForest)\n\nrandomForest 4.7-1.1\nType rfNews() to see new features/changes/bug fixes.\n\nAttaching package: 'randomForest'\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\n\n\nset.seed(3033)\ndf &lt;- read.csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\ndiab_df &lt;- df |&gt;\n  select(Diabetes_binary, HighBP, HighChol, Smoker, BMI, Veggies, HvyAlcoholConsump, DiffWalk, Sex, Age) |&gt;\n  mutate(Diabetes_binary = factor(Diabetes_binary,\n                                  levels = c(0, 1),\n                                  labels = c(\"no_diabetes\", \"prediabetes\"))) |&gt;\n  mutate(HighBP = factor(HighBP,\n                                  levels = c(0, 1),\n                                  labels = c(\"no_high\", \"high_BP\"))) |&gt;\n  mutate(HighChol = factor(HighChol,\n                                  levels = c(0, 1),\n                                  labels = c(\"no_high_cholesterol\", \"high_cholesterol\"))) |&gt;\n\n  mutate(Smoker = factor(Smoker,\n                                  levels = c(0, 1),\n                                  labels = c(\"no\", \"yes\"))) |&gt;\n  \n  mutate(Sex = factor(Sex,\n                                  levels = c(0, 1),\n                                  labels = c(\"female\", \"male\"))) |&gt;\n  mutate(Age = factor(Age,\n                                  levels = c(1:13),\n                                  labels = c(\"Age_18_to_24\", \"Age_25_to_29\", \"Age_30_to_34\", \"Age_35_to_39\", \"Age_40_to_44\", \"Age_45_to_49\", \"Age_50_to_54\", \"Age_55_to_59\", \"Age_60_to_64\", \"Age_65_to_69\", \"Age_70_to_74\", \"Age_75_to_79\", \"Age_80_or_older\")))\n\nintrain &lt;- createDataPartition(y = diab_df$Diabetes_binary, p= 0.7, list = FALSE)\ntraining &lt;- diab_df[intrain,]\ntesting &lt;- diab_df[-intrain,]"
  },
  {
    "objectID": "Modeling.html#logistic-regression-models",
    "href": "Modeling.html#logistic-regression-models",
    "title": "Modeling",
    "section": "3. Logistic Regression Models",
    "text": "3. Logistic Regression Models\nLogistic regression is a statistical model used for binary classification tasks, where the outcome variable is categorical and typically binary (e.g., yes/no, success/failure, 0/1). Unlike linear regression, which predicts a continuous output, logistic regression predicts the probability that a given input belongs to a particular class.\nLogistic regression is particularly well-suited for binary classification problems, making it an excellent choice for many practical applications where the goal is to predict a categorical outcome. Here are some specific reasons why logistic regression might be applied to such data:\n\nBinary Outcomes: If the target variable is binary (e.g., diabetes or not), logistic regression is directly applicable.\nInterpretability: The model’s coefficients provide a clear understanding of how each input feature affects the probability of the outcome.\nBaseline Model: Logistic regression serves as a strong baseline model for binary classification tasks. It is often used as a starting point before exploring more complex models.\nEfficiency: Logistic regression is computationally efficient and scales well with large datasets, making it practical for real-world applications.\nFeature Selection: The simplicity of logistic regression allows for straightforward feature selection and engineering, helping to identify the most important predictors.\n\n\n# Define a custom summary function for log-loss\nlogLossSummary &lt;- function(data, lev = NULL, model = NULL) {\n  if (!is.null(lev)) {\n    levels(data$pred) &lt;- lev\n  }\n  obs &lt;- as.numeric(data$obs) - 1\n  pred &lt;- data[, levels(data$obs)[2]]\n  log_loss_value &lt;- logLoss(obs, pred)\n  c(logLoss = log_loss_value)\n}\n\n# Set up cross-validation\ncontrol &lt;- trainControl(method = \"cv\", \n                        number = 5, \n                        summaryFunction = logLossSummary, \n                        classProbs = TRUE,\n                        savePredictions = TRUE)\n#train models\nset.seed(1011)\nlr_model1 &lt;- train(Diabetes_binary ~., \n                 data = training, \n                 method = \"glm\",\n                 trControl= control,\n                 family = binomial,\n                 metric = \"logLoss\")\n\nset.seed(1011)\nlr_model2 &lt;- train(Diabetes_binary ~ HighBP + HighChol + Smoker + BMI + Sex + Age, \n                 data = training, \n                 method = \"glm\",\n                 trControl= control,\n                 family = binomial,\n                 metric = \"logLoss\")\n\nset.seed(1011)\nlr_model3 &lt;- train(Diabetes_binary ~ HighBP + HighChol + Smoker + BMI + Sex + Age + Sex*Age, \n                 data = training, \n                 method = \"glm\",\n                 trControl= control,\n                 family = binomial,\n                 metric = \"logLoss\")\nsummary(lr_model1)\n\n\nCall:\nNULL\n\nCoefficients:\n                          Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)              -6.311356   0.141732 -44.530  &lt; 2e-16 ***\nHighBPhigh_BP             0.934101   0.017151  54.463  &lt; 2e-16 ***\nHighCholhigh_cholesterol  0.648609   0.015852  40.916  &lt; 2e-16 ***\nSmokeryes                 0.103813   0.015237   6.813 9.56e-12 ***\nBMI                       0.063648   0.001069  59.567  &lt; 2e-16 ***\nVeggies                  -0.180036   0.017831 -10.097  &lt; 2e-16 ***\nHvyAlcoholConsump        -0.895082   0.045024 -19.880  &lt; 2e-16 ***\nDiffWalk                  0.697331   0.017032  40.943  &lt; 2e-16 ***\nSexmale                   0.213306   0.015237  13.999  &lt; 2e-16 ***\nAgeAge_25_to_29           0.143677   0.169520   0.848   0.3967    \nAgeAge_30_to_34           0.378799   0.153340   2.470   0.0135 *  \nAgeAge_35_to_39           0.820082   0.145220   5.647 1.63e-08 ***\nAgeAge_40_to_44           1.021968   0.142222   7.186 6.68e-13 ***\nAgeAge_45_to_49           1.211588   0.140209   8.641  &lt; 2e-16 ***\nAgeAge_50_to_54           1.454527   0.138749  10.483  &lt; 2e-16 ***\nAgeAge_55_to_59           1.545629   0.138278  11.178  &lt; 2e-16 ***\nAgeAge_60_to_64           1.743891   0.137980  12.639  &lt; 2e-16 ***\nAgeAge_65_to_69           1.881372   0.137948  13.638  &lt; 2e-16 ***\nAgeAge_70_to_74           1.941156   0.138364  14.029  &lt; 2e-16 ***\nAgeAge_75_to_79           1.933661   0.139166  13.895  &lt; 2e-16 ***\nAgeAge_80_or_older        1.794351   0.139270  12.884  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 143396  on 177576  degrees of freedom\nResidual deviance: 118638  on 177556  degrees of freedom\nAIC: 118680\n\nNumber of Fisher Scoring iterations: 6\n\nsummary(lr_model2)\n\n\nCall:\nNULL\n\nCoefficients:\n                          Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)              -6.789225   0.142499 -47.644  &lt; 2e-16 ***\nHighBPhigh_BP             0.992919   0.016975  58.492  &lt; 2e-16 ***\nHighCholhigh_cholesterol  0.676444   0.015694  43.103  &lt; 2e-16 ***\nSmokeryes                 0.142769   0.014927   9.565  &lt; 2e-16 ***\nBMI                       0.074049   0.001056  70.139  &lt; 2e-16 ***\nSexmale                   0.139839   0.014912   9.378  &lt; 2e-16 ***\nAgeAge_25_to_29           0.144858   0.171001   0.847   0.3969    \nAgeAge_30_to_34           0.354986   0.154911   2.292   0.0219 *  \nAgeAge_35_to_39           0.841129   0.146548   5.740 9.49e-09 ***\nAgeAge_40_to_44           1.069763   0.143533   7.453 9.12e-14 ***\nAgeAge_45_to_49           1.288800   0.141511   9.107  &lt; 2e-16 ***\nAgeAge_50_to_54           1.556603   0.140057  11.114  &lt; 2e-16 ***\nAgeAge_55_to_59           1.667831   0.139587  11.948  &lt; 2e-16 ***\nAgeAge_60_to_64           1.872873   0.139302  13.445  &lt; 2e-16 ***\nAgeAge_65_to_69           2.005633   0.139280  14.400  &lt; 2e-16 ***\nAgeAge_70_to_74           2.084074   0.139683  14.920  &lt; 2e-16 ***\nAgeAge_75_to_79           2.118730   0.140440  15.086  &lt; 2e-16 ***\nAgeAge_80_or_older        2.064642   0.140482  14.697  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 143396  on 177576  degrees of freedom\nResidual deviance: 120985  on 177559  degrees of freedom\nAIC: 121021\n\nNumber of Fisher Scoring iterations: 6\n\nsummary(lr_model3)\n\n\nCall:\nNULL\n\nCoefficients:\n                               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                  -6.4727280  0.1895406 -34.150  &lt; 2e-16 ***\nHighBPhigh_BP                 1.0011320  0.0170250  58.804  &lt; 2e-16 ***\nHighCholhigh_cholesterol      0.6803918  0.0157151  43.295  &lt; 2e-16 ***\nSmokeryes                     0.1346033  0.0149798   8.986  &lt; 2e-16 ***\nBMI                           0.0737097  0.0010562  69.786  &lt; 2e-16 ***\nSexmale                      -0.4526361  0.2770782  -1.634  0.10234    \nAgeAge_25_to_29               0.1462333  0.2270051   0.644  0.51946    \nAgeAge_30_to_34               0.3196996  0.2074655   1.541  0.12332    \nAgeAge_35_to_39               0.7799933  0.1971794   3.956 7.63e-05 ***\nAgeAge_40_to_44               0.8642999  0.1940541   4.454 8.43e-06 ***\nAgeAge_45_to_49               1.0336292  0.1916962   5.392 6.97e-08 ***\nAgeAge_50_to_54               1.3005789  0.1894025   6.867 6.57e-12 ***\nAgeAge_55_to_59               1.3715949  0.1886652   7.270 3.60e-13 ***\nAgeAge_60_to_64               1.5793941  0.1882518   8.390  &lt; 2e-16 ***\nAgeAge_65_to_69               1.6581361  0.1882130   8.810  &lt; 2e-16 ***\nAgeAge_70_to_74               1.6941786  0.1887552   8.976  &lt; 2e-16 ***\nAgeAge_75_to_79               1.7949223  0.1895200   9.471  &lt; 2e-16 ***\nAgeAge_80_or_older            1.6715157  0.1895272   8.819  &lt; 2e-16 ***\n`Sexmale:AgeAge_25_to_29`    -0.0762189  0.3467899  -0.220  0.82604    \n`Sexmale:AgeAge_30_to_34`     0.0009649  0.3128396   0.003  0.99754    \n`Sexmale:AgeAge_35_to_39`     0.0542548  0.2952317   0.184  0.85419    \n`Sexmale:AgeAge_40_to_44`     0.3766460  0.2885878   1.305  0.19185    \n`Sexmale:AgeAge_45_to_49`     0.4866682  0.2843683   1.711  0.08701 .  \n`Sexmale:AgeAge_50_to_54`     0.4826159  0.2813389   1.715  0.08627 .  \n`Sexmale:AgeAge_55_to_59`     0.5641530  0.2802161   2.013  0.04409 *  \n`Sexmale:AgeAge_60_to_64`     0.5576006  0.2795069   1.995  0.04605 *  \n`Sexmale:AgeAge_65_to_69`     0.6682735  0.2793014   2.393  0.01673 *  \n`Sexmale:AgeAge_70_to_74`     0.7563675  0.2799438   2.702  0.00690 ** \n`Sexmale:AgeAge_75_to_79`     0.6204861  0.2814158   2.205  0.02746 *  \n`Sexmale:AgeAge_80_or_older`  0.7850866  0.2814055   2.790  0.00527 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 143396  on 177576  degrees of freedom\nResidual deviance: 120872  on 177547  degrees of freedom\nAIC: 120932\n\nNumber of Fisher Scoring iterations: 6\n\n# Compare models\nlr_results &lt;- resamples(list(lr_model1, lr_model2, lr_model3))\nsummary(lr_results)\n\n\nCall:\nsummary.resamples(object = lr_results)\n\nModels: Model1, Model2, Model3 \nNumber of resamples: 5 \n\nlogLoss \n            Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nModel1 0.3323953 0.3335058 0.3344196 0.3341737 0.3348244 0.3357233    0\nModel2 0.3400059 0.3402233 0.3407485 0.3407626 0.3408177 0.3420174    0\nModel3 0.3395890 0.3400741 0.3406467 0.3405209 0.3406511 0.3416434    0\n\n\n\nBased on cross validation and logLoss evaluation, the model1 showed lower logLoss score, which indicates better performance. So, the lr_model1 was selected as best logistic regression model.\n\n\nlr_best_model &lt;- lr_model1"
  },
  {
    "objectID": "Modeling.html#classification-tree-model",
    "href": "Modeling.html#classification-tree-model",
    "title": "Modeling",
    "section": "4. Classification Tree Model",
    "text": "4. Classification Tree Model\nIn this part, I will fit a classification tree model, which is a type of decision tree used for classification tasks, where the goal is to predict a categorical target variable. It is a non-parametric supervised learning method that splits the data into subsets based on the value of input features.\nThere are several advantages if we choose the classification tree models: - Interpretability: Classification trees are easy to understand and interpret. They provide a clear visualization of the decision-making process, which is helpful for explaining the model’s decisions to non-technical stakeholders. - Non-Parametric: They do not assume any underlying distribution of the data, making them flexible and capable of handling a wide range of data types and distributions. - Feature Selection: During the splitting process, less important features are effectively ignored, providing an inherent form of feature selection. - Handling Non-Linearity: Trees can capture non-linear relationships between features and the target variable. - Handling Missing Values: Classification trees can handle missing values in both the training and testing data.\n\n#train model\nset.seed(1001)\nct_model &lt;- train(Diabetes_binary ~ ., \n                 data = training, \n                 method = \"rpart\",\n                 trControl= control,\n                 tuneGrid = expand.grid(cp = seq(0.001, 0.1, by = 0.01)),\n                 metric = \"logLoss\")\nprint(ct_model)\n\nCART \n\n177577 samples\n     9 predictor\n     2 classes: 'no_diabetes', 'prediabetes' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142061, 142062, 142062, 142061, 142062 \nResampling results across tuning parameters:\n\n  cp     logLoss  \n  0.001  0.3582282\n  0.011  0.4037576\n  0.021  0.4037576\n  0.031  0.4037576\n  0.041  0.4037576\n  0.051  0.4037576\n  0.061  0.4037576\n  0.071  0.4037576\n  0.081  0.4037576\n  0.091  0.4037576\n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was cp = 0.001.\n\nplot(ct_model)"
  },
  {
    "objectID": "Modeling.html#random-forest-model",
    "href": "Modeling.html#random-forest-model",
    "title": "Modeling",
    "section": "5. Random Forest Model",
    "text": "5. Random Forest Model\nIn this part, I will fit a Random Forest Model. A Random Forest is an ensemble learning method primarily used for classification and regression tasks. It consists of a collection of decision trees (hence “forest”) that work together to improve the model’s performance and robustness. The main idea is to combine the predictions of multiple decision trees to obtain a more accurate and stable prediction.\nCompared to single basic classification tree model, random forest shows the following advantages: - Better Performance: Random Forests generally outperform single decision trees in terms of accuracy and generalization to new data. - Reduced Overfitting: Single decision trees are prone to overfitting, especially when they are deep and complex. Random Forests mitigate this risk by averaging the predictions of multiple trees. - Handling High Dimensionality: Random Forests can handle a large number of features and are effective in identifying the most important features. - Versatility: Random Forests can be used for both classification and regression tasks and are effective across various domains and types of data.\n\n#train model\nset.seed(1001)\nrf_model &lt;- train(Diabetes_binary ~ ., \n                 data = training, \n                 method = \"rf\",\n                 trControl= control,\n                 tuneGrid = expand.grid(mtry = c(1:5)),\n                 metric = \"logLoss\")\nprint(rf_model)\n\nRandom Forest \n\n177577 samples\n     9 predictor\n     2 classes: 'no_diabetes', 'prediabetes' \n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 142061, 142062, 142062, 142061, 142062 \nResampling results across tuning parameters:\n\n  mtry  logLoss\n  1     Inf    \n  2     Inf    \n  3     Inf    \n  4     Inf    \n  5     Inf    \n\nlogLoss was used to select the optimal model using the smallest value.\nThe final value used for the model was mtry = 1."
  },
  {
    "objectID": "Modeling.html#final-model-selection",
    "href": "Modeling.html#final-model-selection",
    "title": "Modeling",
    "section": "6. Final Model Selection",
    "text": "6. Final Model Selection\nNow, I have three best models, lr_best_model, ct_best_model, and rf_best_model. Next, I will predict the test dataset using all the three models, and evaluate the model performance using logLoss as metric.\n\n# predict the testing data\nlr_test_pred &lt;- predict(lr_best_model, newdata = testing, type = \"prob\")\n\nct_test_pred &lt;- predict(ct_model, newdata = testing, type = \"prob\")\n\nrf_test_pred &lt;- predict(rf_model, newdata = testing, type = \"prob\")\n\n# calculate the logLoss value\nobs_value &lt;- as.numeric(testing$Diabetes_binary)\nlr_log_loss_value &lt;- logLoss(obs_value, lr_test_pred$prediabetes)\nct_log_loss_value &lt;- logLoss(obs_value, ct_test_pred$prediabetes)\nrf_log_loss_value &lt;- logLoss(obs_value, rf_test_pred$prediabetes)\n\n\n# Compare models\nfinal_results &lt;- list(lr_log_loss_value, ct_log_loss_value, rf_log_loss_value)\nprint(final_results)\n\n[[1]]\n[1] 2.643875\n\n[[2]]\n[1] 2.42689\n\n[[3]]\n[1] Inf\n\n\nConclusion\nThe logistic regression model return the logLoss score 2.64 for testing dataset, classification tree model returns logLoss score 2.43, while the random forest model returns a infinite logLoss score. We can conclude that the classification tree model shows lower lossLog score, indicates a better prediction performance."
  }
]